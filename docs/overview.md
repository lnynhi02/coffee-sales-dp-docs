# ***COFFEE SALES HYBRID DATA PIPELINE***
    
## **Overview**
---
![Image](img/data-pipeline.png)

## **ğŸ“• Table Of Contents**
* [âš™ï¸ Local Setup](#local-setup)
* [âš¡ Streaming Pipeline](#streaming-pipeline)
* [â³ Batch Pipeline](#batch-pipeline)
* [ğŸ“ Documentation](index.md)
---

## **âš™ï¸ Local Setup**

### 1. Prerequisites
- <p>Install <a href="https://www.docker.com/products/docker-desktop/" target="_blank">Docker</a>.</p>
- <p>Install <a href="https://www.python.org/" target="_blank">Python</a>.</p>
- <p>Install <a href="https://www.postgresql.org/download/" target="_blank">PostgreSQL</a>.</p>
- <p>Install <a href="https://www.mongodb.com/docs/manual/administration/install-community/" target="_blank">self-hosted MongoDB</a> or <a href="https://www.mongodb.com/atlas" target="_blank">MongoDB in Cloud</a>.</p>
- <p>Create an <a href="https://www.cloud.airbyte.com/" target="_blank">Airbyte account</a>.</p>
- Clone, fork, or download this GitHub repository on your local machine using the following command:
```bash
git clone https://github.com/lnynhi02/coffee-sales-data-pipeline.git
```

### 2. Project Structure
```
coffee-sales-data-pipeline/
â”œâ”€â”€ connectors/                  # Kafka Connect configuration files
â”‚   â”œâ”€â”€ elasticsearch-sink-connector.json  # Sink connector for Elasticsearch
â”‚   â”œâ”€â”€ generate-mongo-connector.py        # Script to create MongoDB source connector
â”œâ”€â”€ dbt-project/                 # DBT project for data transformation
â”‚   â”œâ”€â”€ coffee_dw/               # Data warehouse managed by DBT
â”‚   â”‚   â”œâ”€â”€ seeds/               # Static seed data
â”‚   â”‚   â”‚   â”œâ”€â”€ payment_method/  # Payment method details (CSV, schema)
â”‚   â”‚   â”‚   â”œâ”€â”€ product/         # Product details & categories (CSV, schema)
â”‚   â”‚   â”‚   â”œâ”€â”€ store/           # Store details (CSV, schema)
â”‚   â”‚   â”œâ”€â”€ models/              # DBT models
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/         # Load raw data from PostgreSQL
â”‚   â”‚   â”‚   â”œâ”€â”€ marts/           # Transformed data models
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ dimensions/  # Product, store, payment_method
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ facts/       # Sales data
â”‚   â”œâ”€â”€ dbt_project.yml          # DBT project config
â”‚   â”œâ”€â”€ packages.yml             # DBT dependencies
â”‚   â”œâ”€â”€ profiles.yml             # DBT database connection config
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ mongodb_data.py           # Streams data to MongoDB
â”œâ”€â”€ docker-compose.yml            # Docker Compose for service orchestration
â”œâ”€â”€ config.ini                    # MongoDB connection details
â””â”€â”€ run-pipeline.ps1              # Script to run the pipeline
```

---

1. Create a virtual environment:
   ```sh
   python -m venv venv
   ```
2. Activate the virtual environment:
      - **Windows PowerShell:**
      ```sh
      venv\Scripts\Activate
      ```
      - **Linux/macOS:**
      ```sh
      source venv/bin/activate
      ```
3. Install the necessary package:
   ```sh
   pip install pymongo
   ```
---

## **âš¡ Streaming Pipeline**
Visit [this](streaming.md) for more details.

## **â³ Batch Pipeline**
Visit [this](batch.md) for more details.

## **ğŸ“ Documentation**
Visit [this](index.md) for more details.
